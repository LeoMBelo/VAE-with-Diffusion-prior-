{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41299298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d266799",
   "metadata": {},
   "source": [
    "# Diffusion prior in VAE\n",
    "* Estrutura similar ao DCGAN\n",
    "* KL Annealing Linear\n",
    "\n",
    "## Parte 1: DDPM prior\n",
    "\n",
    "### a) Beta schedule cossine\n",
    "A ideia é definir coeficientes de variância ($\\beta_t$) de forma que o produto cumulativo dos $\\alpha_t = 1 - \\beta_t$ seja suavizado.\n",
    "\n",
    "É trabalhado como $\\bar{\\alpha}_t = cos (\\frac{t/T + s}{1+s} \\times \\frac{\\pi}{2})^2$, onde $s$ é um pequeno coeficiente para não começar do valor zero. Depois, os betas são calculados como $\\beta_t = 1 - \\frac{\\bar{\\alpha_{t+1}}}{\\bar{\\alpha}_t}$\n",
    "\n",
    "Com isso, temos:\n",
    "\n",
    "    * Garantia de transições suaves: Cada etapa adiciona uma quantidade pequena de ruído de forma controlada, o que é crucial para a validade das aproximações gaussianas nos DDPM.\n",
    "\n",
    "    * Melhora na qualidade das amostras: resulta em melhores métricas de qualidade (como FID), possivelmente porque a distribuição de ruído se comporta de maneira mais gradual.\n",
    "\n",
    "* Referência: \n",
    "    * Improved Denoising Diffusion Probabilistic Models (2021)\n",
    "\n",
    "### b) Timestep embedding\n",
    "* O objetivo é gerar uma representação contínua dos timesteps (por exemplo, 0, 1, …, T-1), convertendo eles em vetores contínuos por meio de funções trigonométricas (seno e cosseno)\n",
    "\n",
    "* ***Equações do Positional encoding***:\n",
    "Para uma posição $pos$ e uma dimensão $i$ do embedding:\n",
    "$$ \n",
    "PE(pos,2i) = sin(\\frac{pos}{1000^{2i/d_{model}}})\n",
    "$$\n",
    "    \n",
    "$$ \n",
    "PE(pos,2i+1) = cos(\\frac{pos}{1000^{2i/d_{model}}}),\n",
    "$$\n",
    "\n",
    "onde:\n",
    "    * $pos$ seria o timestep (ou a posição) que queremos codificar;\n",
    "    * $d_{model}$ é a dimensão total do embedding;\n",
    "    * e para cada par de dimensões (uma para o seno e outra para o cosseno), o denominador $1000^{2i/d_{model}}$ garante que cada dimensão do embedding corresponde a uma frequência diferente.\n",
    "    \n",
    "* A codificação sinusoidal permite que o modelo saiba a posição (ou, no nosso caso, o timestep) usando funções senoidais e cosenoidais em diferentes frequências.\n",
    "\n",
    "* Referências:\n",
    "    * Denoising Diffusion Probabilistic Models (2020)\n",
    "    * Attention is all you need (2023)\n",
    "\n",
    "### c) DDPM loss\n",
    "* Para cada amostra de z0 (latent extraído pelo encoder):\n",
    "    1. Amostra um timestep t aleatório (de 0 a T-1).\n",
    "    2. Calcula z_t = sqrt(prod_alpha[t]) * z0 + sqrt(1 - prod_alpha[t]) * epsilon,\n",
    "         onde epsilon é ruído gaussiano.\n",
    "    3. O modelo diffusion_prior tenta prever esse epsilon a partir de z_t e t.\n",
    "    4. A loss é o erro quadrático médio (MSE) entre o ruído previsto e o ruído real.\n",
    "    \n",
    "* Equação do artigo:\n",
    "$$ L_{DDPM} (x_0, \\phi) = E_{t,x_0,x_t} [\\frac{1}{2 \\sigma_t ^2} ||\\mu_\\phi(x_t,t) - \\tilde{\\mu}_t(x_0,x_t)||^2],\n",
    "$$\n",
    "onde $\\tilde{\\mu}_t(x_0,x_t)$ é a média de $q(x_{t-1}|x_0,x_t)$, a forward diffusion posterior condicionada na observação $x_0$, $\\mu_\\phi(x_t,t)$ média prevista pelo modelo para o processo reverso e $\\sigma_t ^2$ é a variância associada ao passo t.\n",
    "\n",
    "\n",
    "* Efetuaremos uma reparametrização para usar o ruído no lugar de $\\mu$, ou seja, se parametrizarmos o processo reverso de forma adequada, essa diferença entre as médias pode ser reescrita como a diferença entre o ruído real $\\epsilon$ e uma predição do modelo: $||\\mu_\\phi(x_t,t) - \\tilde{\\mu}_t(x_0,x_t)||^2 \\propto ||\\epsilon_\\phi(x_t,t) - \\epsilon||^2$, onde $\\epsilon_\\phi(x_t,t)$ é a predição do ruído pelo modelo.\n",
    "    * A reparametrização usada é $z_t = \\sqrt{\\Pi_{s=0} ^t \\alpha_s}z_0 + \\sqrt{1-\\Pi_{s=0} ^t \\alpha_s} \\epsilon$, onde $z_0$ é a amostra do espaço latente (obtida do encoder), $\\alpha_s = 1 - \\beta_s$ (note que $\\alpha_s$ representa a proporção da informação original que permanece após a adição de ruído naquele passo) e $\\epsilon$ é amostrado de $N(0,I)$.\n",
    "    * Tal formulação permite que a amostragem seja diferenciável.\n",
    "    * Dessa forma, ao treinar o modelo para prever o ruído $\\epsilon$ (por meio do MSE), garantimos que a aprendizagem está focada em como remover o ruído do estado atual $z_t$.\n",
    "    \n",
    "    \n",
    "## Parte 2: VAE\n",
    "\n",
    "### a) Encoder\n",
    "* Responsável por levar os dados até o espaço latente.\n",
    "* Usaremos duas redes convolucionais e calcularemos, usando camadas lineares, $\\mu$ e $log \\sigma^2$ da distribuição $q(z|x)$\n",
    "* As duas saídas serão usadas para a reparametrização, onde amostramos $z$ a partir de $q(z|x)$.\n",
    "\n",
    "\n",
    "### b) Decoder\n",
    "* Responsável por levar do espaço latente até a reconstrução de imagens $(\\hat{x})$.\n",
    "* Primeiro, o vetor $z$ é transformado por uma camada linear e, em seguida, é desachatado.\n",
    "* Usaremos duas redes convolucionais e usamos a função de ativação sigmoid de modo a garantir uma saída no intervalo [0,1], assim como estão as imagens normalizadas. \n",
    "\n",
    "\n",
    "### c) Reparametrização\n",
    "* Usamos a equação $z = \\mu(x) + \\sigma(x) \\odot \\epsilon$, ou seja, $z_i = \\mu_i + \\sigma_i . \\epsilon_i$\n",
    "\n",
    "## Parte 3: Loss treinamento\n",
    "* No artigo principal, a loss é dada por:\n",
    "$$ \\mathcal{L}(x;\\phi, \\theta, \\psi) = E_{q_\\psi}[log \\frac{p_\\theta(x|z)}{q_\\psi (z|x)}] + E_{q_\\psi} [L_{DDPM}(z_0;\\phi)],\n",
    "$$\n",
    "onde $q_\\psi(z|x)$ é a distribuição aproximada do encoder, $p_\\theta (x|z)$ é o modelo de verossimilhança do decoder.\n",
    "\n",
    "* De certo modo, a loss é escrita da forma: $Loss = reconstructed_{Loss} + Latent_{Loss}$.\n",
    "    * A ***recon_loss*** é calculada como a Binary Cross-Entropy entre a imagem reconstruída e a imagem original.\n",
    "    * A ***latent_loss***  é calculada a partir da loss de difusão. Essa parte substitui o termo tradicional da KL divergence.\n",
    "    \n",
    "* Lembrando que a função loss de difusão faz:\n",
    "    1. A partir de $z_0$, é gerado o $z_t$ usando o forward do DDPM: $z_t = \\sqrt{\\Pi_{s=0} ^t \\alpha_s}z_0 + \\sqrt{1-\\Pi_{s=0} ^t \\alpha_s} \\epsilon$, com $\\epsilon \\sim N(0,I)$.\n",
    "    2. O diffusion prior $(\\epsilon_\\phi(z_t,t))$ tenta prever o ruído $\\epsilon$.\n",
    "    3. A loss é o erro quadrático médio (MSE) entre a predição do ruído e o ruído real: $DDPM_{loss} = ||\\epsilon_\\phi(z_t,t) - \\epsilon||^2$\n",
    "    \n",
    "### Uso de KL Annealing\n",
    "A técnica consiste em iniciar o treinamento com o termo da divergência KL com um peso baixo – ou até mesmo zero – e aumentá-lo gradativamente até atingir o valor desejado. À medida que o treinamento avança, o peso em KL é aumentado, forçando o modelo a regularizar o espaço latente para que ele se aproxime da distribuição do prior.\n",
    "\n",
    "Vale destacar que valores altos do peso para o KL no caso linear, influencia termo KL a ter mais influência. Isso força o encoder a manter o posterior mais próximo do prior. Por outro lado, menores valores para o peso permite que o encoder se concentre mais na reconstrução dos dados e aprenda representações mais detalhadas e ricas, mas se a regularização for muito fraca, o espaço latente pode não ter uma estrutura bem definida\n",
    "\n",
    "* Referências:\n",
    "    * Generating Sentences from a Continuous Space\n",
    "    * Understanding Posterior Collapse in Generative Latent Variable Models\n",
    "    \n",
    "## Parte 4: Arquitetura similar a DCGAN\n",
    "No Encoder a arquitetura utiliza camadas convolucionais com strides maiores (geralmente stride 2) para fazer o downsampling das imagens, seguida de técnicas como Batch Normalization e funções de ativação (normalmente LeakyReLU). Essa estrutura é típica do encoder em DCGAN.\n",
    "\n",
    "No Decoder realiza-se o processo inverso, normalmente começando com uma camada totalmente conectada para \"descompactar\" o vetor latente em mapas de características (feature maps) e, em seguida, aplicando camadas de convolução transposta (ou upsampling seguidas de convolução), além de Batch Normalization e funções de ativação (como ReLU ou Tanh na camada final) para gerar as imagens.\n",
    "\n",
    "\n",
    "\n",
    "* Referência:\n",
    "    * UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parâmetros Gerais e Hiperparâmetros \n",
    "hidden_dim = 128\n",
    "time_embed_dim = 100\n",
    "latent_dim = 64\n",
    "T = 100\n",
    "latent_weight = 0.1  # peso para o termo KLD (com annealing)\n",
    "epochs = 400  # número total de épocas para o treinamento conjunto\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Annealing schedule para o KLD\n",
    "def latent_weight(epoch, max_epochs):\n",
    "    annealing_epochs = max_epochs * 0.2\n",
    "    if epoch < annealing_epochs:\n",
    "        return latent_weight * (epoch / annealing_epochs)\n",
    "    return latent_weight\n",
    "\n",
    "\n",
    "# Transformação para CIFAR10\n",
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# Cosine Beta Schedule\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float32)\n",
    "    alphas_prod = torch.cos(((x/timesteps) + s) / (1+s) * (math.pi * 0.5))**2\n",
    "    alphas_prod = alphas_prod / alphas_prod[0]\n",
    "    betas = 1 - (alphas_prod[1:] / alphas_prod[:-1])\n",
    "    betas = betas.clamp(0, 0.999)\n",
    "    return betas\n",
    "\n",
    "betas = cosine_beta_schedule(T).to(device)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "\n",
    "# Função de embedding para timestep\n",
    "def timestep_embedding(timesteps, embedding_dim):\n",
    "    half_dim = embedding_dim // 2 \n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) *\n",
    "                    -(math.log(10000.0) / (half_dim - 1)))\n",
    "    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = torch.cat([emb, torch.zeros(timesteps.size(0), 1, device=emb.device)], dim=1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# Diffusion Prior\n",
    "class DiffusionPrior(nn.Module):\n",
    "    def __init__(self, latent_dim, time_embed_dim=100, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        A rede recebe um vetor ruidoso z_t com dimensão = latent_dim \n",
    "        e concatena com o embedding do timestep (dimensão time_embed_dim),\n",
    "        formando a entrada de dimensão (latent_dim + time_embed_dim).\n",
    "        \"\"\"\n",
    "        super(DiffusionPrior, self).__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        # Refinando o vetor timestep antes de concatenar com z_t\n",
    "            ## Uso de SiLU para gerar gradientes mais suaves do que ReLU\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, time_embed_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim * 2, time_embed_dim)\n",
    "        )\n",
    "        # A entrada é concatenada: [z_t (latent_dim) , t_embed (time_embed_dim)]\n",
    "        self.fc1 = nn.Linear(latent_dim + time_embed_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim) #normalizaçâo na camada\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        #  Essa saída representa a predição do ruído (ou correção) que deve ser subtraído de z_t\n",
    "            ## Lembrete: Ao retirar o ruído de z_t geramos z_0\n",
    "            ## A saída deve ter dimensão igual a latent_dim\n",
    "        self.fc4 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Objetivo: manter a variância das ativações (e dos gradientes) aproximadamente constante através das camadas\n",
    "            ## Ajuda a evitar tanto a explosão quanto o desaparecimento dos gradientes durante o treinamento.\n",
    "            ## Treinamento mais estável\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc4.weight)\n",
    "    \n",
    "    def forward(self, z_t, t):\n",
    "        # Lembrete: z_t é o ruidoso e t é o time_step atual\n",
    "        # Cria o embedding do timestep com dimensão time_embed_dim\n",
    "            ## Usa as funções sinusoidais\n",
    "        t_embed = timestep_embedding(t, self.time_embed_dim)\n",
    "        t_embed = self.time_mlp(t_embed)\n",
    "        # Concatena a informação do vetor ruidoso com o embedding\n",
    "        x = torch.cat([z_t, t_embed], dim=1)\n",
    "        h1 = F.silu(self.norm1(self.fc1(x))) #1ª representaçâo interna (h_1)\n",
    "        h2 = F.silu(self.norm2(self.fc2(h1)))\n",
    "        h3 = F.silu(self.norm3(self.fc3(h2)))\n",
    "        # Conexão residual simples\n",
    "            ## Essa soma ajuda a preservar informações iniciais e melhora a propagação do gradiente durante o treinamento\n",
    "        h = h3 + h1  \n",
    "        # Predição do ruìdo\n",
    "        noise_pred = self.fc4(h)\n",
    "        return noise_pred\n",
    "\n",
    "\n",
    "# Loss DDPM \n",
    "def ddpm_loss(diffusion_prior, z0, betas, alpha_bars, T):\n",
    "    batch_size, latent_dim = z0.shape\n",
    "    device = z0.device\n",
    "    # Seleciona um timestep aleatório para cada exemplo\n",
    "    t = torch.randint(0, T, (batch_size,), device=device)\n",
    "    alpha_bar_t = alpha_bars[t].view(-1, 1)\n",
    "    epsilon = torch.randn_like(z0)\n",
    "    # Cria z_t de acordo com a fórmula do forward process\n",
    "    z_t = torch.sqrt(alpha_bar_t) * z0 + torch.sqrt(1 - alpha_bar_t) * epsilon\n",
    "    noise_pred = diffusion_prior(z_t, t)\n",
    "    beta_t = betas[t].view(-1, 1)\n",
    "    # MSE ponderado por 1/(2 * beta_t)\n",
    "    loss = torch.mean((noise_pred - epsilon)**2 / (2 * beta_t))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Encoder para CIFAR10 (inspirado no DCGAN)\n",
    "    ## Dimensão CIFAR: 32 x 32\n",
    "class EncoderCIFAR10(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        O encoder extrai características via camadas convolucionais\n",
    "        e, no final do processo, utiliza camadas lineares para produzir os parâmetros do espaço\n",
    "        latente (μ e log(σ²)) com dimensão igual a latent_dim.\n",
    "        \"\"\"\n",
    "        super(EncoderCIFAR10, self).__init__()\n",
    "        # Conv 1\n",
    "            ## Entrada: 3 canais (RGB)\n",
    "            ## Saìda: 64 canais\n",
    "            ## Calculo: saida = [(dim_img + 2xpadding - kernel_size)/stride] + 1\n",
    "                ### No primeiro caso, dim_img = 32 -> Resultado 16\n",
    "                ### dim_saida_conv1 = (64,16,16)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Conv 2\n",
    "            ## Entrada: 64 canais \n",
    "            ## Saìda: 128 canais\n",
    "            ## Calculo: saida = [(dim_img + 2xpadding - kernel_size)/stride] + 1\n",
    "                ### No segundo caso, dim_img = 16 -> Resultado 8\n",
    "                ### dim_saida_conv1 = (128,8,8)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # Conv 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # Flatten\n",
    "        self.fc_input_dim = 256 * 4 * 4\n",
    "        # Aprender \\mu e log \\sigma^2\n",
    "        self.fc_mu = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Dimensão de x: (batch_size,3, 32, 32)\n",
    "        batch_size = x.size(0) \n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        x = x.view(batch_size, -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# Decoder para CIFAR10 (inspirado no DCGAN)\n",
    "class DecoderCIFAR10(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        O decoder recebe o vetor latente de dimensão latent_dim e o mapeia\n",
    "        para uma representação espacial, seguido de \n",
    "        camadas deconvolucionais para reconstruir a imagem original.\n",
    "        \"\"\"\n",
    "        super(DecoderCIFAR10, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 256 * 4 * 4) #Olhar dim encoder\n",
    "        # DeConv1\n",
    "            ## dim_saida = (dim_img_entrada - 1) x stride - 2 x padding + kernel_size\n",
    "            ## dim_img_entrada = 4 -> dim_saida = 8\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        # DeConv2\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # DeConv3\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # A camada fc transforma vetor latente (batch, latent_dim) no vetor (batch, 256*4*4)\n",
    "        batch_size = z.size(0)\n",
    "        x = F.relu(self.fc(z))\n",
    "        #O vetor é reorganizado (reshape) para formar um tensor com formato (batch,256, 4, 4)\n",
    "        x = x.view(batch_size, 256, 4, 4)\n",
    "        # Processo de Desconvolução\n",
    "        x = F.relu(self.bn1(self.deconv1(x)))\n",
    "        x = F.relu(self.bn2(self.deconv2(x)))\n",
    "        x = torch.tanh(self.deconv3(x)) #Garante intervalo [-1,1]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Reparametrização para amostrar z a partir de mu e logvar\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "# Função de Sampling \n",
    "    ## (Reverse Diffusion) para gerar latentes\n",
    "\n",
    "def sample_latent(diffusion_prior, T, latent_dim, betas, alpha_bars, device):\n",
    "    z = torch.randn((1, latent_dim), device=device) #Amostra z_T (o estado final do forward process) a ser “desruído” no processo reverso.\n",
    "    # O loop aplica o processo de reverse diffusion para atualizar z passo a passo\n",
    "    for t in reversed(range(1, T)): \n",
    "        t_tensor = torch.full((z.shape[0],), t, device=device, dtype=torch.long) #informa em qual passo de difusão está\n",
    "        pred_noise = diffusion_prior(z, t_tensor) #pred \\epsilon\n",
    "        beta_t = betas[t] #variância do ruído para o passo atual\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_bars[t] #valor cumulativo\n",
    "        # Essa operação \"remove\" parte do ruído predito\n",
    "            ## aproximando z de um estado com menos ruído.\n",
    "        z = (z - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * pred_noise) / torch.sqrt(alpha_t) #atualiza z\n",
    "        \n",
    "        # Add ruìdo ao z atualizado\n",
    "            ## simula a parte aleatória do reverse process\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(z)\n",
    "            z = z + torch.sqrt(beta_t) * noise\n",
    "    return z\n",
    "\n",
    "\n",
    "# Inicialização \n",
    "encoder = EncoderCIFAR10(latent_dim).to(device)\n",
    "decoder = DecoderCIFAR10(latent_dim).to(device)\n",
    "diffusion_prior = DiffusionPrior(latent_dim).to(device)\n",
    "\n",
    "# Otimizador conjunto para encoder, decoder e diffusion prior\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()) + list(diffusion_prior.parameters()),\n",
    "    lr=5e-4\n",
    ")\n",
    "\n",
    "# Carregamento CIFAR10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar10)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Treinamento Conjunto\n",
    "def train(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    diffusion_prior.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        recon_loss_total = 0.0\n",
    "        kld_loss_total = 0.0\n",
    "        ddpm_loss_total = 0.0\n",
    "        \n",
    "        for x, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward no VAE\n",
    "            mu, logvar = encoder(x)\n",
    "            z0 = reparameterize(mu, logvar)\n",
    "            x_recon = decoder(z0)\n",
    "            \n",
    "            recon_loss = F.mse_loss(x_recon, x, reduction='sum') / x.size(0)\n",
    "            kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "            \n",
    "            # Loss do diffusion prior (DDPM) aplicada sobre z0\n",
    "            ddpm_loss_val = ddpm_loss(diffusion_prior, z0, betas, alpha_bars, T)\n",
    "            \n",
    "            current_latent_weight = latent_weight(epoch, epochs)\n",
    "            # Loss total do modelo conjunto, conforme Eq. (20) do artigo:\n",
    "            loss = recon_loss + current_latent_weight * kld_loss + ddpm_loss_val\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kld_loss_total += kld_loss.item()\n",
    "            ddpm_loss_total += ddpm_loss_val.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_recon = recon_loss_total / len(train_loader)\n",
    "        avg_kld = kld_loss_total / len(train_loader)\n",
    "        avg_ddpm = ddpm_loss_total / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Recon: {avg_recon:.4f} | KLD: {avg_kld:.4f} | DDPM: {avg_ddpm:.4f}\")\n",
    "\n",
    "print(\"=== Treinando VAE com Diffusion Prior (Treinamento Conjunto) ===\")\n",
    "train(epochs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11520264",
   "metadata": {},
   "source": [
    "Geração e avaliação de imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a67df6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Geração\n",
    "num_imgs = 2000\n",
    "def generate_img(num_images=num_imgs):\n",
    "    fake_folder = './fake_images_cifar10'\n",
    "    os.makedirs(fake_folder, exist_ok=True)\n",
    "    \n",
    "    diffusion_prior.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_samples = []\n",
    "        for _ in range(num_images):\n",
    "            z = sample_latent(diffusion_prior, T, latent_dim, betas, alpha_bars, device)\n",
    "            z_samples.append(z)\n",
    "        z_samples = torch.cat(z_samples, dim=0)\n",
    "        images = decoder(z_samples).cpu()\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        utils.save_image(img, os.path.join(fake_folder, f\"fake_cifa10_{i}.png\"))\n",
    "\n",
    "generate_img(num_imgs)\n",
    "\n",
    "\n",
    "# Visualização de Amostras\n",
    "def visualize_samples(num_samples=10):\n",
    "    diffusion_prior.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            z = sample_latent(diffusion_prior, T, latent_dim, betas, alpha_bars, device)\n",
    "            z_samples.append(z)\n",
    "        z_samples = torch.cat(z_samples, dim=0)\n",
    "        images = decoder(z_samples).cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i].permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples()\n",
    "\n",
    "# Salva imagens reais (opcional, se ainda não foram salvas)\n",
    "def real_images(dataset, folder, num_images=num_imgs):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    count = 0\n",
    "    for img, _ in dataset:\n",
    "        fname = f\"real_{count}.png\"\n",
    "        utils.save_image(img, os.path.join(folder, fname))\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "real_images(test_dataset, './real_images_cifar10', num_images=num_imgs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o FID\n",
    "def fid():\n",
    "    from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "    real_path = './real_images_cifar10'\n",
    "    fake_path = './fake_images_cifar10'\n",
    "    \n",
    "    fid_batch_size = batch_size if batch_size > 0 else 1\n",
    "    \n",
    "    try:\n",
    "        fid = calculate_fid_given_paths([real_path, fake_path], fid_batch_size, device, dims=2048)\n",
    "        return fid\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao calcular FID:\", str(e))\n",
    "        return float('nan')\n",
    "\n",
    "fid_value = fid()\n",
    "print(\"\\nFID Score:\", fid_value)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
